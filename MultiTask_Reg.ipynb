{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T17:01:19.988792Z",
     "iopub.status.busy": "2025-11-22T17:01:19.988024Z",
     "iopub.status.idle": "2025-11-22T17:01:20.664920Z",
     "shell.execute_reply": "2025-11-22T17:01:20.664196Z",
     "shell.execute_reply.started": "2025-11-22T17:01:19.988768Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0: Importing libraries and setting up environment...\n",
      "TensorFlow version: 2.18.0\n",
      "  -> imports done\n",
      "\n",
      "GPU devices found: ['/physical_device:GPU:0', '/physical_device:GPU:1']\n",
      "STEP 0 ready.\n"
     ]
    }
   ],
   "source": [
    "# ------------------ STEP 0: Imports, seeds, checks & helpers ------------------\n",
    "print(\"STEP 0: Importing libraries and setting up environment...\")\n",
    "\n",
    "import os\n",
    "# reduce TF logging spam\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "\n",
    "import math\n",
    "import random\n",
    "import gc\n",
    "import sys\n",
    "from glob import glob\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# sklearn utils\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# TensorFlow import\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    tf_version = tf.__version__\n",
    "    print(\"TensorFlow version:\", tf_version)\n",
    "except Exception as e:\n",
    "    tf = None\n",
    "    print(\"Warning: TensorFlow import failed. Exception:\", e)\n",
    "    raise e\n",
    "\n",
    "print(\"  -> imports done\\n\")\n",
    "\n",
    "# ------------------ Seeds and reproducibility ------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if tf is not None:\n",
    "    tf.random.set_seed(SEED)\n",
    "\n",
    "# ------------------ Filesystem & environment checks ------------------\n",
    "def ensure_dir(d):\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# ------------------ GPU/TPU setup ------------------\n",
    "if tf is not None:\n",
    "    try:\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            for g in gpus:\n",
    "                tf.config.experimental.set_memory_growth(g, True)\n",
    "            print(f\"GPU devices found: {[g.name for g in gpus]}\")\n",
    "        else:\n",
    "            print(\"No GPU devices found (running on CPU).\")\n",
    "    except Exception as e:\n",
    "        print(\"Warning checking GPU devices:\", e)\n",
    "\n",
    "print(\"STEP 0 ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T17:01:20.668927Z",
     "iopub.status.busy": "2025-11-22T17:01:20.668605Z",
     "iopub.status.idle": "2025-11-22T17:01:20.765721Z",
     "shell.execute_reply": "2025-11-22T17:01:20.765152Z",
     "shell.execute_reply.started": "2025-11-22T17:01:20.668908Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Setting configuration...\n",
      "  DATA_PATH: /kaggle/input/multiclass-dataset\n",
      "  NUM_CLIENTS: 6\n",
      "STEP 1 ready.\n"
     ]
    }
   ],
   "source": [
    "# ------------------ STEP 1: Configuration & Hyperparameters ------------------\n",
    "print(\"STEP 1: Setting configuration...\")\n",
    "\n",
    "DATA_PATH = \"/kaggle/input/multiclass-dataset\" # OR \"/kaggle/working/multiclass-dataset\"\n",
    "TRAIN_FILE = os.path.join(DATA_PATH, \"final_train_multiclass.parquet\")\n",
    "TEST_FILE = os.path.join(DATA_PATH, \"final_test_multiclass.parquet\")\n",
    "\n",
    "# --- Model Architecture ---\n",
    "D_MODEL = 128            # Embedding dimension for the model\n",
    "NUM_HEADS = 4            # Number of attention heads (if USE_MHA=True)\n",
    "USE_MHA = False          # Set to True to use Multi-Head Attention\n",
    "\n",
    "# --- MODIFIED: Added T3 config ---\n",
    "COND_DIM_T2 = 16         # Dimension of the conditioning vector from T1 -> T2\n",
    "HEAD2_HIDDEN = 64        # Hidden layer size for the Task 2 head\n",
    "COND_DIM_T3 = 16         # NEW: Dimension of the conditioning vector from T2 -> T3\n",
    "HEAD3_HIDDEN = 64        # NEW: Hidden layer size for the Task 3 head\n",
    "\n",
    "# --- Training ---\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# --- Federated Learning ---\n",
    "NUM_CLIENTS = 6          # Number of clients\n",
    "NUM_ROUNDS = 10          # Number of federated rounds\n",
    "EPOCHS_PER_CLIENT_ROUND = 1 # Local epochs each client runs per round\n",
    "\n",
    "# --- Sanity Checks ---\n",
    "if not os.path.isdir(DATA_PATH):\n",
    "    print(f\"WARNING: DATA_PATH '{DATA_PATH}' does not exist.\")\n",
    "if D_MODEL % NUM_HEADS != 0:\n",
    "    print(f\"WARNING: D_MODEL ({D_MODEL}) not divisible by NUM_HEADS ({NUM_HEADS}).\")\n",
    "\n",
    "print(f\"  DATA_PATH: {DATA_PATH}\")\n",
    "print(f\"  NUM_CLIENTS: {NUM_CLIENTS}\")\n",
    "print(\"STEP 1 ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T17:01:20.766663Z",
     "iopub.status.busy": "2025-11-22T17:01:20.766397Z",
     "iopub.status.idle": "2025-11-22T17:01:21.228151Z",
     "shell.execute_reply": "2025-11-22T17:01:21.227346Z",
     "shell.execute_reply.started": "2025-11-22T17:01:20.766620Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read file from: /kaggle/input/multiclass-dataset/final_test_multiclass.parquet\n",
      "Successfully read file.\n",
      "Detected 'label' as the label column.\n",
      "\n",
      "Found 45 features.\n",
      "Copy the list block below into your STEP 2 cell:\n",
      "\n",
      "# --- START: Copy this list ---\n",
      "['Header_Length',\n",
      " 'Protocol Type',\n",
      " 'Duration',\n",
      " 'Rate',\n",
      " 'Srate',\n",
      " 'Drate',\n",
      " 'fin_flag_number',\n",
      " 'syn_flag_number',\n",
      " 'rst_flag_number',\n",
      " 'psh_flag_number',\n",
      " 'ack_flag_number',\n",
      " 'ece_flag_number',\n",
      " 'cwr_flag_number',\n",
      " 'ack_count',\n",
      " 'syn_count',\n",
      " 'fin_count',\n",
      " 'rst_count',\n",
      " 'HTTP',\n",
      " 'HTTPS',\n",
      " 'DNS',\n",
      " 'Telnet',\n",
      " 'SMTP',\n",
      " 'SSH',\n",
      " 'IRC',\n",
      " 'TCP',\n",
      " 'UDP',\n",
      " 'DHCP',\n",
      " 'ARP',\n",
      " 'ICMP',\n",
      " 'IGMP',\n",
      " 'IPv',\n",
      " 'LLC',\n",
      " 'Tot sum',\n",
      " 'Min',\n",
      " 'Max',\n",
      " 'AVG',\n",
      " 'Std',\n",
      " 'Tot size',\n",
      " 'IAT',\n",
      " 'Number',\n",
      " 'Magnitue',\n",
      " 'Radius',\n",
      " 'Covariance',\n",
      " 'Variance',\n",
      " 'Weight']\n",
      "# --- END: Copy this list ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import pprint\n",
    "import os\n",
    "\n",
    "# --- Define the *correct* file path here ---\n",
    "# Use the same DATA_PATH as your main notebook\n",
    "DATA_PATH = \"/kaggle/input/multiclass-dataset\" \n",
    "file_path = os.path.join(DATA_PATH, \"final_test_multiclass.parquet\") \n",
    "\n",
    "print(f\"Attempting to read file from: {file_path}\")\n",
    "\n",
    "try:\n",
    "    # Use 'pyarrow' as it's common in Kaggle environments\n",
    "    df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "    print(\"Successfully read file.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Could not read parquet file at '{file_path}'. {e}\")\n",
    "    print(\"Please ensure this path is correct and 'pyarrow' or 'fastparquet' is installed.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "all_columns = df.columns.tolist()\n",
    "\n",
    "# --- Try to find the label column ---\n",
    "label_col = None\n",
    "# This is the most common name in your notebook\n",
    "possible_label_names = {'label', 'Label', 'labels', 'CLASS', 'class', 'target', 'y'}\n",
    "\n",
    "if 'label' in all_columns:\n",
    "    label_col = 'label'\n",
    "else:\n",
    "    for col in all_columns:\n",
    "        if col.lower() in possible_label_names:\n",
    "            label_col = col\n",
    "            break\n",
    "\n",
    "if label_col:\n",
    "    print(f\"Detected '{label_col}' as the label column.\")\n",
    "    # Exclude the label column from the feature list\n",
    "    feature_list = [col for col in all_columns if col != label_col]\n",
    "else:\n",
    "    print(\"WARNING: Could not auto-detect a label column (e.g., 'label').\")\n",
    "    print(\"The list below might include it. Please remove it manually.\")\n",
    "    feature_list = all_columns\n",
    "\n",
    "print(f\"\\nFound {len(feature_list)} features.\")\n",
    "print(\"Copy the list block below into your STEP 2 cell:\")\n",
    "print(\"\\n# --- START: Copy this list ---\")\n",
    "pprint.pprint(feature_list)\n",
    "print(\"# --- END: Copy this list ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T17:01:21.230121Z",
     "iopub.status.busy": "2025-11-22T17:01:21.229907Z",
     "iopub.status.idle": "2025-11-22T17:01:21.236241Z",
     "shell.execute_reply": "2025-11-22T17:01:21.235538Z",
     "shell.execute_reply.started": "2025-11-22T17:01:21.230103Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: Defining feature list...\n",
      "  Defined 44 features to keep.\n",
      "STEP 2 ready.\n"
     ]
    }
   ],
   "source": [
    "# ------------------ STEP 2: Define Feature List ------------------\n",
    "print(\"STEP 2: Defining feature list...\")\n",
    "\n",
    "features_to_keep = [\n",
    " 'Header_Length',\n",
    " 'Protocol Type',\n",
    " 'Duration',\n",
    " 'Rate',\n",
    " 'Srate',\n",
    " 'Drate',\n",
    " 'fin_flag_number',\n",
    " 'syn_flag_number',\n",
    " 'rst_flag_number',\n",
    " 'psh_flag_number',\n",
    " 'ack_flag_number',\n",
    " 'ece_flag_number',\n",
    " 'cwr_flag_number',\n",
    " 'ack_count',\n",
    " 'syn_count',\n",
    " 'fin_count',\n",
    " 'rst_count',\n",
    " 'HTTP',\n",
    " 'HTTPS',\n",
    " 'DNS',\n",
    " 'Telnet',\n",
    " 'SMTP',\n",
    " 'SSH',\n",
    " 'IRC',\n",
    " 'TCP',\n",
    " 'UDP',\n",
    " 'DHCP',\n",
    " 'ARP',\n",
    " 'ICMP',\n",
    " 'IGMP',\n",
    " 'IPv',\n",
    " 'LLC',\n",
    " 'Tot sum',\n",
    " 'Min',\n",
    " 'Max',\n",
    " 'AVG',\n",
    " 'Std',\n",
    " 'Tot size',\n",
    " 'IAT',\n",
    " 'Number',\n",
    " # 'Magnitue',  <-- REMOVED: This is now your TARGET (y) for Task 3\n",
    " 'Radius',\n",
    " 'Covariance',\n",
    " 'Variance',\n",
    " 'Weight'\n",
    "]\n",
    "\n",
    "if not features_to_keep:\n",
    "     print(\"WARNING: 'features_to_keep' is empty or was not pasted.\")\n",
    "     print(\"Please run the script from the previous step and paste the real feature list.\")\n",
    "\n",
    "print(f\"  Defined {len(features_to_keep)} features to keep.\")\n",
    "print(\"STEP 2 ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T17:01:21.237237Z",
     "iopub.status.busy": "2025-11-22T17:01:21.236963Z",
     "iopub.status.idle": "2025-11-22T17:01:21.255966Z",
     "shell.execute_reply": "2025-11-22T17:01:21.255215Z",
     "shell.execute_reply.started": "2025-11-22T17:01:21.237210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 3: Label mapping (T1 Family / T2 Subtype / T3 Magnitude)...\n",
      "  Task 3 will predict 'Magnitue' (Regression).\n",
      "  num_classes_task1=6, num_classes_task2=19, Task3=Regression\n",
      "STEP 3 ready.\n"
     ]
    }
   ],
   "source": [
    "# ------------------ STEP 3: Label mapping ------------------\n",
    "print(\"STEP 3: Label mapping (T1 Family / T2 Subtype / T3 Magnitude)...\")\n",
    "\n",
    "# --- task1 (family) mapping ---\n",
    "class_map_task1 = {\n",
    "    0: 'Benign', 1: 'Spoofing', 2: 'DDoS', 3: 'DoS', 4: 'Malformed', 5: 'Reconnaissance'\n",
    "}\n",
    "num_classes_task1 = len(class_map_task1)\n",
    "\n",
    "# --- task2 (subtype) mapping ---\n",
    "class_map_task2 = {\n",
    "    0: 'Benign', 1: 'ARP_Spoofing', 2: 'MQTT-DDoS-Connect', 3: 'MQTT-DDoS-Publish',\n",
    "    4: 'MQTT-DoS-Connect', 5: 'MQTT-DoS-Publish', 6: 'MQTT-Malformed', 7: 'Recon-OS_Scan',\n",
    "    8: 'Recon-Ping_Sweep', 9: 'Recon-Port_Scan', 10: 'Recon-VulScan', 11: 'TCP_IP-DDoS-ICMP',\n",
    "    12: 'TCP_IP-DDoS-SYN', 13: 'TCP_IP-DDoS-TCP', 14: 'TCP_IP-DDoS-UDP', 15: 'TCP_IP-DoS-ICMP',\n",
    "    16: 'TCP_IP-DoS-SYN', 17: 'TCP_IP-DoS-TCP', 18: 'TCP_IP-DoS-UDP'\n",
    "}\n",
    "num_classes_task2 = len(class_map_task2)\n",
    "BENIGN_IDX = 0 \n",
    "\n",
    "# --- remap original fine-grained labels (0..18) -> task1 family (0..5) ---\n",
    "label_remapping = {\n",
    "    0: 0, 1: 1, 2: 2, 3: 2, 4: 3, 5: 3, 6: 4, 7: 5, 8: 5, 9: 5,\n",
    "    10: 5, 11: 2, 12: 2, 13: 2, 14: 2, 15: 3, 16: 3, 17: 3, 18: 3\n",
    "}\n",
    "\n",
    "# --- Task 3 (Regression) Config ---\n",
    "# We don't map classes. We define the target column name.\n",
    "T3_TARGET_COL = 'Magnitue'  # Note: Matches the spelling in your file\n",
    "print(f\"  Task 3 will predict '{T3_TARGET_COL}' (Regression).\")\n",
    "\n",
    "print(f\"  num_classes_task1={num_classes_task1}, num_classes_task2={num_classes_task2}, Task3=Regression\")\n",
    "print(\"STEP 3 ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T17:01:21.256941Z",
     "iopub.status.busy": "2025-11-22T17:01:21.256779Z",
     "iopub.status.idle": "2025-11-22T17:01:39.357543Z",
     "shell.execute_reply": "2025-11-22T17:01:39.356887Z",
     "shell.execute_reply.started": "2025-11-22T17:01:21.256928Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: Locating parquet files, loading and preprocessing...\n",
      "Loading train parquet: /kaggle/input/multiclass-dataset/final_train_multiclass.parquet\n",
      "Loading test parquet: /kaggle/input/multiclass-dataset/final_test_multiclass.parquet\n",
      "  Loaded train rows: 7160831, test rows: 1614182\n",
      "  Using 44 features.\n",
      "  Filling NaNs in features and scaling X...\n",
      "  Scaling Task 3 target ('Magnitue')...\n",
      "  Computing class weights...\n",
      "  Preprocessing complete.\n",
      "    train_X: (7160831, 44), y1: (7160831,), y2: (7160831,), y3: (7160831,)\n",
      "STEP 4 done.\n"
     ]
    }
   ],
   "source": [
    "# ------------------ STEP 4: Loading and preprocessing data ------------------\n",
    "print(\"STEP 4: Locating parquet files, loading and preprocessing...\")\n",
    "\n",
    "# Load train data\n",
    "if os.path.exists(TRAIN_FILE):\n",
    "    print(f\"Loading train parquet: {TRAIN_FILE}\")\n",
    "    train_df = pd.read_parquet(TRAIN_FILE, engine='pyarrow')\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Train file not found at: {TRAIN_FILE}\")\n",
    "\n",
    "# Load test data\n",
    "if os.path.exists(TEST_FILE):\n",
    "    print(f\"Loading test parquet: {TEST_FILE}\")\n",
    "    test_df = pd.read_parquet(TEST_FILE, engine='pyarrow')\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Test file not found at: {TEST_FILE}\")\n",
    "\n",
    "print(f\"  Loaded train rows: {len(train_df)}, test rows: {len(test_df)}\")\n",
    "\n",
    "# Validate features\n",
    "missing_cols = [c for c in features_to_keep if c not in train_df.columns]\n",
    "if missing_cols:\n",
    "    print(\"WARNING: The following requested features are missing in train:\", missing_cols)\n",
    "    features = [c for c in features_to_keep if c in train_df.columns]\n",
    "    if not features:\n",
    "        raise ValueError(\"No features found. Did you update 'features_to_keep' in STEP 2?\")\n",
    "else:\n",
    "    features = features_to_keep[:]\n",
    "print(f\"  Using {len(features)} features.\")\n",
    "\n",
    "# Detect Label Columns\n",
    "label_col_t2 = 'label' # Contains 0-18\n",
    "if label_col_t2 not in train_df.columns:\n",
    "    raise ValueError(f\"Missing '{label_col_t2}' column for classification labels.\")\n",
    "\n",
    "# Check for Regression Target\n",
    "if T3_TARGET_COL not in train_df.columns:\n",
    "    raise ValueError(f\"Missing '{T3_TARGET_COL}' column for Task 3 regression.\")\n",
    "\n",
    "# --- MAP LABELS ---\n",
    "def map_labels(df):\n",
    "    df2 = df.copy()\n",
    "    df2['orig_label_t2'] = df2[label_col_t2].astype(int)\n",
    "    \n",
    "    # Task 1: Map T2 label -> T1 family\n",
    "    df2['y_task1'] = df2['orig_label_t2'].map(label_remapping).astype(int)\n",
    "    \n",
    "    # Task 2: Subtype\n",
    "    df2['y_task2'] = df2['orig_label_t2'].astype(int)\n",
    "    \n",
    "    # Task 3: Regression Target (Magnitude)\n",
    "    # Fill NaNs in magnitude with 0.0 just in case\n",
    "    df2['y_task3'] = df2[T3_TARGET_COL].fillna(0.0).astype(float)\n",
    "\n",
    "    # --- MASKING LOGIC ---\n",
    "    mask = (df2['y_task1'] == BENIGN_IDX)\n",
    "    \n",
    "    # Task 2: Mask Benign as -1 (Loss function will ignore these)\n",
    "    df2.loc[mask, 'y_task2'] = -1\n",
    "    \n",
    "    # Task 3: For Benign, FORCE Magnitude to 0.0 (Do NOT mask)\n",
    "    # This teaches the model: \"Benign traffic has 0 intensity.\"\n",
    "    df2.loc[mask, 'y_task3'] = 0.0\n",
    "    \n",
    "    return df2\n",
    "\n",
    "train_df = map_labels(train_df)\n",
    "test_df  = map_labels(test_df)\n",
    "\n",
    "# --- SCALING ---\n",
    "print(\"  Filling NaNs in features and scaling X...\")\n",
    "train_median = train_df[features].median()\n",
    "train_X = train_df[features].fillna(train_median).values.astype('float32')\n",
    "test_X  = test_df[features].fillna(train_median).values.astype('float32')\n",
    "\n",
    "# Scale Features (X)\n",
    "scaler = MinMaxScaler().fit(train_X)\n",
    "train_X = scaler.transform(train_X)\n",
    "test_X  = scaler.transform(test_X)\n",
    "\n",
    "# Scale Regression Target (y3)\n",
    "# It is good practice to scale regression targets to [0,1] or [-1,1]\n",
    "print(f\"  Scaling Task 3 target ('{T3_TARGET_COL}')...\")\n",
    "train_y3_raw = train_df['y_task3'].values.reshape(-1, 1)\n",
    "test_y3_raw  = test_df['y_task3'].values.reshape(-1, 1)\n",
    "\n",
    "scaler_y3 = MinMaxScaler().fit(train_y3_raw)\n",
    "train_y3 = scaler_y3.transform(train_y3_raw).flatten().astype('float32')\n",
    "test_y3  = scaler_y3.transform(test_y3_raw).flatten().astype('float32')\n",
    "\n",
    "# Prepare Classification Labels\n",
    "train_y1 = train_df['y_task1'].values.astype('int32')\n",
    "train_y2 = train_df['y_task2'].values.astype('int32')\n",
    "test_y1  = test_df['y_task1'].values.astype('int32')\n",
    "test_y2  = test_df['y_task2'].values.astype('int32')\n",
    "\n",
    "# --- CLASS WEIGHTS ---\n",
    "print(\"  Computing class weights...\")\n",
    "# Task 1\n",
    "family_class_weights = compute_class_weight('balanced', classes=np.arange(num_classes_task1), y=train_y1)\n",
    "family_class_weights_tf = tf.convert_to_tensor(family_class_weights, dtype=tf.float32)\n",
    "\n",
    "# Task 2\n",
    "y2_valid = train_y2[train_y2 != -1]\n",
    "if len(y2_valid) > 0:\n",
    "    subtype_classes = np.unique(y2_valid)\n",
    "    subtype_class_weights = compute_class_weight('balanced', classes=subtype_classes, y=y2_valid)\n",
    "    full_subtype_weights = np.zeros(num_classes_task2, dtype=np.float32)\n",
    "    full_subtype_weights[subtype_classes] = subtype_class_weights\n",
    "else:\n",
    "    full_subtype_weights = np.ones(num_classes_task2, dtype=np.float32)\n",
    "subtype_class_weights_tf = tf.convert_to_tensor(full_subtype_weights, dtype=tf.float32)\n",
    "\n",
    "# Task 3 (Regression) -> No class weights needed!\n",
    "\n",
    "print(\"  Preprocessing complete.\")\n",
    "print(f\"    train_X: {train_X.shape}, y1: {train_y1.shape}, y2: {train_y2.shape}, y3: {train_y3.shape}\")\n",
    "print(\"STEP 4 done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T17:01:39.358683Z",
     "iopub.status.busy": "2025-11-22T17:01:39.358384Z",
     "iopub.status.idle": "2025-11-22T17:01:45.741936Z",
     "shell.execute_reply": "2025-11-22T17:01:45.741141Z",
     "shell.execute_reply.started": "2025-11-22T17:01:39.358663Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 5: Creating non-IID client partitions (X, y1, y2, y3)...\n",
      "  Partitioning data for 6 families across 6 clients.\n",
      "  Client 0: samples=553000, T1 dist=[134912, 963, 298832, 111761, 308, 6224], Avg Mag=0.1412\n",
      "  Client 1: samples=439920, T1 dist=[11564, 11232, 298831, 111761, 308, 6224], Avg Mag=0.1891\n",
      "  Client 2: samples=3617182, T1 dist=[11564, 963, 3486362, 111761, 308, 6224], Avg Mag=0.1835\n",
      "  Client 3: samples=1621763, T1 dist=[11564, 963, 298831, 1303874, 308, 6223], Avg Mag=0.1880\n",
      "  Client 4: samples=432932, T1 dist=[11564, 963, 298831, 111761, 3590, 6223], Avg Mag=0.1847\n",
      "  Client 5: samples=496034, T1 dist=[11564, 963, 298831, 111760, 308, 72608], Avg Mag=0.1861\n",
      "STEP 5 done.\n"
     ]
    }
   ],
   "source": [
    "# ------------------ STEP 5: Creating non-IID client partitions ------------------\n",
    "print(\"STEP 5: Creating non-IID client partitions (X, y1, y2, y3)...\")\n",
    "\n",
    "def partition_non_iid(X, y1, y2, y3, num_clients, seed=SEED):\n",
    "    if num_clients < 1:\n",
    "        raise ValueError(\"num_clients must be >= 1\")\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    N, feat_dim = X.shape\n",
    "    # Initialize clients with empty lists for all 4 arrays\n",
    "    clients = [dict(X=[], y1=[], y2=[], y3=[]) for _ in range(num_clients)]\n",
    "    \n",
    "    unique_fams = np.unique(y1)\n",
    "    family_idx = {int(f): np.where(y1 == f)[0] for f in unique_fams}\n",
    "    fams = sorted(family_idx.keys())\n",
    "\n",
    "    print(f\"  Partitioning data for {len(fams)} families across {num_clients} clients.\")\n",
    "\n",
    "    for i, fam in enumerate(fams):\n",
    "        idxs = family_idx[fam].copy()\n",
    "        if idxs.size == 0: continue\n",
    "        rng.shuffle(idxs)\n",
    "        \n",
    "        primary_client = i % num_clients\n",
    "        n = len(idxs)\n",
    "\n",
    "        # Split Logic\n",
    "        n_primary = max(1, int(0.7 * n)) if num_clients > 1 else n\n",
    "        primary_idxs = idxs[:n_primary]\n",
    "        rest_idxs = idxs[n_primary:]\n",
    "        \n",
    "        # Helper to append data\n",
    "        def append_to_client(c_idx, indices):\n",
    "            clients[c_idx]['X'].append(X[indices])\n",
    "            clients[c_idx]['y1'].append(y1[indices])\n",
    "            clients[c_idx]['y2'].append(y2[indices])\n",
    "            clients[c_idx]['y3'].append(y3[indices]) # <--- ADDED y3\n",
    "\n",
    "        # Assign Primary\n",
    "        append_to_client(primary_client, primary_idxs)\n",
    "\n",
    "        # Distribute Rest\n",
    "        if num_clients > 1 and rest_idxs.size > 0:\n",
    "            other_clients = [c for c in range(num_clients) if c != primary_client]\n",
    "            splits = np.array_split(rest_idxs, len(other_clients))\n",
    "            for j, r in enumerate(splits):\n",
    "                if r.size > 0:\n",
    "                    cidx = other_clients[j]\n",
    "                    append_to_client(cidx, r)\n",
    "\n",
    "    # Finalize\n",
    "    for c in clients:\n",
    "        if c['X']:\n",
    "            c['X'] = np.vstack(c['X']).astype(np.float32)\n",
    "            c['y1'] = np.concatenate(c['y1']).astype(np.int32)\n",
    "            c['y2'] = np.concatenate(c['y2']).astype(np.int32)\n",
    "            c['y3'] = np.concatenate(c['y3']).astype(np.float32) # <--- ADDED y3\n",
    "            \n",
    "            # Shuffle\n",
    "            perm = rng.permutation(c['y1'].shape[0])\n",
    "            c['X'] = c['X'][perm]\n",
    "            c['y1'] = c['y1'][perm]\n",
    "            c['y2'] = c['y2'][perm]\n",
    "            c['y3'] = c['y3'][perm] # <--- ADDED y3\n",
    "        else:\n",
    "            c['X'] = np.zeros((0, feat_dim), dtype=np.float32)\n",
    "            c['y1'] = np.zeros((0,), dtype=np.int32)\n",
    "            c['y2'] = np.zeros((0,), dtype=np.int32)\n",
    "            c['y3'] = np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "    return clients\n",
    "\n",
    "# Run partitioning\n",
    "clients = partition_non_iid(train_X, train_y1, train_y2, train_y3, NUM_CLIENTS, seed=SEED)\n",
    "\n",
    "# Print client stats\n",
    "for i, c in enumerate(clients):\n",
    "    n = c['y1'].shape[0]\n",
    "    if n > 0:\n",
    "        counts = np.bincount(c['y1'], minlength=num_classes_task1)\n",
    "        # Calculate avg magnitude for this client\n",
    "        avg_mag = np.mean(c['y3'])\n",
    "    else:\n",
    "        counts = np.zeros(num_classes_task1, dtype=int)\n",
    "        avg_mag = 0.0\n",
    "    print(f\"  Client {i}: samples={n}, T1 dist={counts.tolist()}, Avg Mag={avg_mag:.4f}\")\n",
    "\n",
    "print(\"STEP 5 done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T17:01:45.743163Z",
     "iopub.status.busy": "2025-11-22T17:01:45.742886Z",
     "iopub.status.idle": "2025-11-22T17:01:46.756926Z",
     "shell.execute_reply": "2025-11-22T17:01:46.756165Z",
     "shell.execute_reply.started": "2025-11-22T17:01:45.743138Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6: Building the joint model (FIXED: Sigmoid for 0-1 Regression)...\n",
      "Building light model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"joint_multitask_model_light\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"joint_multitask_model_light\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ features            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,760</span> │ features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ shared_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ logits_family       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │ shared_embedding… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span> │ logits_family[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ shared_embedding… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,280</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ logits_subtype      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,235</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ logits_subtype[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ shared_embedding… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,280</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_magnitude    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ features            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m5,760\u001b[0m │ features[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m256\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ shared_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m256\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ logits_family       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │        \u001b[38;5;34m774\u001b[0m │ shared_embedding… │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │        \u001b[38;5;34m112\u001b[0m │ logits_family[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m144\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ shared_embedding… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m9,280\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ logits_subtype      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        │      \u001b[38;5;34m1,235\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │        \u001b[38;5;34m320\u001b[0m │ logits_subtype[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m144\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ shared_embedding… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m9,280\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_magnitude    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">76,874</span> (300.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m76,874\u001b[0m (300.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">76,874</span> (300.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m76,874\u001b[0m (300.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 6 done.\n"
     ]
    }
   ],
   "source": [
    "# ------------------ STEP 6: Model building (Keras) ------------------\n",
    "print(\"STEP 6: Building the joint model (FIXED: Sigmoid for 0-1 Regression)...\")\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def build_joint_model_light(input_dim: int,\n",
    "                            d_model: int = D_MODEL,\n",
    "                            cond_dim_t2: int = COND_DIM_T2,\n",
    "                            head2_hidden: int = HEAD2_HIDDEN,\n",
    "                            cond_dim_t3: int = COND_DIM_T3,\n",
    "                            head3_hidden: int = HEAD3_HIDDEN,\n",
    "                            num_families: int = num_classes_task1,\n",
    "                            num_subtypes: int = num_classes_task2) -> tf.keras.Model:\n",
    "    \"\"\"Lightweight model with 3 heads.\"\"\"\n",
    "    inp = layers.Input(shape=(input_dim,), name='features')\n",
    "    \n",
    "    # Shared Backbone\n",
    "    x = layers.Dense(d_model, activation=tf.nn.gelu, kernel_initializer='glorot_uniform')(inp)\n",
    "    r = layers.Dense(d_model, activation=tf.nn.gelu, kernel_initializer='glorot_uniform')(x)\n",
    "    r = layers.Dense(d_model, activation=None, kernel_initializer='glorot_uniform')(r)\n",
    "    x = layers.Add()([x, r])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    shared = layers.Dense(d_model, activation=tf.nn.gelu, kernel_initializer='glorot_uniform')(x)\n",
    "    shared = layers.LayerNormalization(name='shared_embedding')(shared)\n",
    "\n",
    "    # --- Task 1 Head (Family) ---\n",
    "    logits1 = layers.Dense(num_families, name='logits_family')(shared)\n",
    "    \n",
    "    # --- Task 2 Head (Subtype) ---\n",
    "    cond1 = layers.Dense(cond_dim_t2, activation='linear')(logits1)\n",
    "    h2_in = layers.Concatenate(axis=-1)([shared, cond1])\n",
    "    h2 = layers.Dense(head2_hidden, activation='relu', kernel_initializer='glorot_uniform')(h2_in)\n",
    "    logits2 = layers.Dense(num_subtypes, name='logits_subtype')(h2)\n",
    "\n",
    "    # --- Task 3 Head (Magnitude) ---\n",
    "    cond2 = layers.Dense(cond_dim_t3, activation='linear')(logits2)\n",
    "    h3_in = layers.Concatenate(axis=-1)([shared, cond2])\n",
    "    h3 = layers.Dense(head3_hidden, activation='relu', kernel_initializer='glorot_uniform')(h3_in)\n",
    "    \n",
    "    # <<< FIX: Use 'sigmoid' to force output between 0.0 and 1.0 >>>\n",
    "    output3 = layers.Dense(1, activation='sigmoid', name='output_magnitude')(h3)\n",
    "\n",
    "    return tf.keras.Model(inputs=inp, outputs=[logits1, logits2, output3], name='joint_multitask_model_light')\n",
    "\n",
    "\n",
    "def build_joint_model_with_mha(input_dim: int,\n",
    "                               d_model: int = D_MODEL,\n",
    "                               cond_dim_t2: int = COND_DIM_T2,\n",
    "                               head2_hidden: int = HEAD2_HIDDEN,\n",
    "                               cond_dim_t3: int = COND_DIM_T3,\n",
    "                               head3_hidden: int = HEAD3_HIDDEN,\n",
    "                               num_families: int = num_classes_task1,\n",
    "                               num_subtypes: int = num_classes_task2,\n",
    "                               num_heads: int = NUM_HEADS) -> tf.keras.Model:\n",
    "    \"\"\"MHA Model with 3 heads.\"\"\"\n",
    "    if d_model % num_heads != 0:\n",
    "        raise ValueError(f\"d_model ({d_model}) must be divisible by num_heads ({num_heads}).\")\n",
    "    inp = layers.Input(shape=(input_dim,), name='features')\n",
    "    \n",
    "    # Shared Backbone\n",
    "    x = layers.Dense(d_model, activation=tf.nn.gelu, kernel_initializer='glorot_uniform')(inp)\n",
    "    x_seq = layers.Reshape((1, d_model))(x)\n",
    "    attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads, dropout=0.1)(x_seq, x_seq)\n",
    "    attn = layers.Reshape((d_model,))(attn)\n",
    "    x = layers.Add()([x, attn])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    shared = layers.Dense(d_model, activation=tf.nn.gelu, kernel_initializer='glorot_uniform')(x)\n",
    "    shared = layers.LayerNormalization(name='shared_embedding')(shared)\n",
    "\n",
    "    # --- Task 1 Head ---\n",
    "    logits1 = layers.Dense(num_families, name='logits_family')(shared)\n",
    "\n",
    "    # --- Task 2 Head ---\n",
    "    cond1 = layers.Dense(cond_dim_t2, activation='linear')(logits1)\n",
    "    h2_in = layers.Concatenate(axis=-1)([shared, cond1])\n",
    "    h2 = layers.Dense(head2_hidden, activation='relu', kernel_initializer='glorot_uniform')(h2_in)\n",
    "    logits2 = layers.Dense(num_subtypes, name='logits_subtype')(h2)\n",
    "\n",
    "    # --- Task 3 Head ---\n",
    "    cond2 = layers.Dense(cond_dim_t3, activation='linear')(logits2)\n",
    "    h3_in = layers.Concatenate(axis=-1)([shared, cond2])\n",
    "    h3 = layers.Dense(head3_hidden, activation='relu', kernel_initializer='glorot_uniform')(h3_in)\n",
    "    \n",
    "    # <<< FIX: Use 'sigmoid' to force output between 0.0 and 1.0 >>>\n",
    "    output3 = layers.Dense(1, activation='sigmoid', name='output_magnitude')(h3)\n",
    "\n",
    "    return tf.keras.Model(inputs=inp, outputs=[logits1, logits2, output3], name='joint_multitask_model_mha')\n",
    "\n",
    "# ------------------- Build -------------------\n",
    "input_dim = int(train_X.shape[1])\n",
    "if USE_MHA:\n",
    "    print(\"Building MHA model...\")\n",
    "    model = build_joint_model_with_mha(input_dim)\n",
    "else:\n",
    "    print(\"Building light model...\")\n",
    "    model = build_joint_model_light(input_dim)\n",
    "\n",
    "model.summary()\n",
    "print(\"STEP 6 done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T17:01:46.758251Z",
     "iopub.status.busy": "2025-11-22T17:01:46.758007Z",
     "iopub.status.idle": "2025-11-22T17:01:48.453508Z",
     "shell.execute_reply": "2025-11-22T17:01:48.452677Z",
     "shell.execute_reply.started": "2025-11-22T17:01:46.758233Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 7: Define losses (Weighted for Sigmoid Regression)...\n",
      "STEP 7 ready.\n"
     ]
    }
   ],
   "source": [
    "# ------------------ STEP 7: Loss functions & trainer helpers ------------------\n",
    "print(\"STEP 7: Define losses (Weighted for Sigmoid Regression)...\")\n",
    "\n",
    "# --- Task 1 Weights & Loss ---\n",
    "family_class_weights = compute_class_weight('balanced', classes=np.arange(num_classes_task1), y=train_y1)\n",
    "family_class_weights_tf = tf.convert_to_tensor(family_class_weights, dtype=tf.float32)\n",
    "loss_fn_task1 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "def compute_task1_loss(logits, y_true):\n",
    "    # per-sample CE (shape (B,))\n",
    "    per_sample = loss_fn_task1(y_true, logits)\n",
    "    # gather per-sample class weights\n",
    "    weights = tf.gather(family_class_weights_tf, tf.cast(y_true, tf.int32))\n",
    "    # Apply weights\n",
    "    weighted = per_sample * weights\n",
    "    return tf.reduce_mean(weighted)\n",
    "\n",
    "# --- Task 2 Weights & Loss (Masked) ---\n",
    "# `subtype_class_weights_tf` was created in STEP 4\n",
    "loss_fn_task2 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "def compute_task2_loss(logits, y_true):\n",
    "    \"\"\"\n",
    "    Computes weighted, masked loss for Task 2.\n",
    "    Ignores samples where y_true == -1.\n",
    "    \"\"\"\n",
    "    # Create a boolean mask for valid (non -1) labels\n",
    "    mask_bool = tf.not_equal(y_true, -1)\n",
    "    mask = tf.cast(mask_bool, tf.float32)\n",
    "\n",
    "    # Create a \"safe\" y_true to index weights, replacing -1 with 0\n",
    "    safe_y = tf.where(mask_bool, y_true, tf.zeros_like(y_true))\n",
    "\n",
    "    # Calculate per-sample loss\n",
    "    per_sample_loss = loss_fn_task2(safe_y, logits)  # (B,)\n",
    "    \n",
    "    # Get per-sample weights\n",
    "    per_sample_weights = tf.gather(subtype_class_weights_tf, safe_y)\n",
    "\n",
    "    # Apply both weights and mask\n",
    "    weighted_masked_loss = per_sample_loss * per_sample_weights * mask\n",
    "\n",
    "    # Normalize by the number of *valid* samples in the batch\n",
    "    denom = tf.reduce_sum(mask) + 1e-8\n",
    "    loss = tf.reduce_sum(weighted_masked_loss) / denom\n",
    "    n_valid = tf.reduce_sum(mask)\n",
    "\n",
    "    return loss, n_valid\n",
    "\n",
    "# --- Task 3 Loss (Regression) ---\n",
    "# We use Mean Squared Error (MSE). Since output is Sigmoid (0-1), this works well.\n",
    "loss_fn_task3 = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "\n",
    "def compute_task3_loss(preds, y_true):\n",
    "    # Ensure y_true is float and shaped correctly (B, 1)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_true = tf.reshape(y_true, (-1, 1))\n",
    "    return loss_fn_task3(y_true, preds)\n",
    "\n",
    "# --- Optimizer ---\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# --- TF.Function for training ---\n",
    "@tf.function\n",
    "def train_step(x_batch, y1_batch, y2_batch, y3_batch, model_to_train, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        l1_logits, l2_logits, out3 = model_to_train(x_batch, training=True)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss1 = compute_task1_loss(l1_logits, y1_batch)\n",
    "        loss2, n_valid = compute_task2_loss(l2_logits, y2_batch)\n",
    "        loss3 = compute_task3_loss(out3, y3_batch)\n",
    "        \n",
    "        # Total loss\n",
    "        # We multiply loss3 by 10.0 to give it more importance, \n",
    "        # since MSE on 0-1 values is naturally very small compared to CrossEntropy.\n",
    "        loss = loss1 + loss2 + (10.0 * loss3)\n",
    "        \n",
    "    grads = tape.gradient(loss, model_to_train.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model_to_train.trainable_variables))\n",
    "    return loss1, loss2, loss3, n_valid\n",
    "\n",
    "# --- Model weight helpers ---\n",
    "def get_model_weights(model):\n",
    "    return model.get_weights()\n",
    "\n",
    "def set_model_weights(model, weights):\n",
    "    model.set_weights(weights)\n",
    "\n",
    "print(\"STEP 7 ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T17:01:48.465277Z",
     "iopub.status.busy": "2025-11-22T17:01:48.465049Z",
     "iopub.status.idle": "2025-11-22T17:46:25.366540Z",
     "shell.execute_reply": "2025-11-22T17:46:25.365813Z",
     "shell.execute_reply.started": "2025-11-22T17:01:48.465259Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 8: Federated simulation - FedAvg across clients...\n",
      "Client 0: dataset prepared, samples=553000\n",
      "Client 1: dataset prepared, samples=439920\n",
      "Client 2: dataset prepared, samples=3617182\n",
      "Client 3: dataset prepared, samples=1621763\n",
      "Client 4: dataset prepared, samples=432932\n",
      "Client 5: dataset prepared, samples=496034\n",
      "\n",
      "Starting Federated Training (T1+T2+T3)...\n",
      "\n",
      "=== Fed Round 1/10 ===\n",
      "  Client 0 training...\n",
      "  Client 1 training...\n",
      "  Client 2 training...\n",
      "  Client 3 training...\n",
      "  Client 4 training...\n",
      "  Client 5 training...\n",
      "  Aggregation complete.\n",
      "  Eval Round 1: T1 Acc=0.7511 | T2 F1=0.4976 | T3 MSE=0.00348\n",
      "  Improved T2 macro-F1: -1.0000 -> 0.4976. Saving model...\n",
      "\n",
      "=== Fed Round 2/10 ===\n",
      "  Client 0 training...\n",
      "  Client 1 training...\n",
      "  Client 2 training...\n",
      "  Client 3 training...\n",
      "  Client 4 training...\n",
      "  Client 5 training...\n",
      "  Aggregation complete.\n",
      "  Eval Round 2: T1 Acc=0.7563 | T2 F1=0.4527 | T3 MSE=0.00083\n",
      "\n",
      "=== Fed Round 3/10 ===\n",
      "  Client 0 training...\n",
      "  Client 1 training...\n",
      "  Client 2 training...\n",
      "  Client 3 training...\n",
      "  Client 4 training...\n",
      "  Client 5 training...\n",
      "  Aggregation complete.\n",
      "  Eval Round 3: T1 Acc=0.7553 | T2 F1=0.4836 | T3 MSE=0.00059\n",
      "\n",
      "=== Fed Round 4/10 ===\n",
      "  Client 0 training...\n",
      "  Client 1 training...\n",
      "  Client 2 training...\n",
      "  Client 3 training...\n",
      "  Client 4 training...\n",
      "  Client 5 training...\n",
      "  Aggregation complete.\n",
      "  Eval Round 4: T1 Acc=0.7548 | T2 F1=0.4791 | T3 MSE=0.00068\n",
      "\n",
      "=== Fed Round 5/10 ===\n",
      "  Client 0 training...\n",
      "  Client 1 training...\n",
      "  Client 2 training...\n",
      "  Client 3 training...\n",
      "  Client 4 training...\n",
      "  Client 5 training...\n",
      "  Aggregation complete.\n",
      "  Eval Round 5: T1 Acc=0.7533 | T2 F1=0.4985 | T3 MSE=0.00058\n",
      "  Improved T2 macro-F1: 0.4976 -> 0.4985. Saving model...\n",
      "\n",
      "=== Fed Round 6/10 ===\n",
      "  Client 0 training...\n",
      "  Client 1 training...\n",
      "  Client 2 training...\n",
      "  Client 3 training...\n",
      "  Client 4 training...\n",
      "  Client 5 training...\n",
      "  Aggregation complete.\n",
      "  Eval Round 6: T1 Acc=0.7644 | T2 F1=0.5091 | T3 MSE=0.00068\n",
      "  Improved T2 macro-F1: 0.4985 -> 0.5091. Saving model...\n",
      "\n",
      "=== Fed Round 7/10 ===\n",
      "  Client 0 training...\n",
      "  Client 1 training...\n",
      "  Client 2 training...\n",
      "  Client 3 training...\n",
      "  Client 4 training...\n",
      "  Client 5 training...\n",
      "  Aggregation complete.\n",
      "  Eval Round 7: T1 Acc=0.7678 | T2 F1=0.4862 | T3 MSE=0.00061\n",
      "\n",
      "=== Fed Round 8/10 ===\n",
      "  Client 0 training...\n",
      "  Client 1 training...\n",
      "  Client 2 training...\n",
      "  Client 3 training...\n",
      "  Client 4 training...\n",
      "  Client 5 training...\n",
      "  Aggregation complete.\n",
      "  Eval Round 8: T1 Acc=0.7713 | T2 F1=0.4981 | T3 MSE=0.00055\n",
      "\n",
      "=== Fed Round 9/10 ===\n",
      "  Client 0 training...\n",
      "  Client 1 training...\n",
      "  Client 2 training...\n",
      "  Client 3 training...\n",
      "  Client 4 training...\n",
      "  Client 5 training...\n",
      "  Aggregation complete.\n",
      "  Eval Round 9: T1 Acc=0.7241 | T2 F1=0.5478 | T3 MSE=0.00051\n",
      "  Improved T2 macro-F1: 0.5091 -> 0.5478. Saving model...\n",
      "\n",
      "=== Fed Round 10/10 ===\n",
      "  Client 0 training...\n",
      "  Client 1 training...\n",
      "  Client 2 training...\n",
      "  Client 3 training...\n",
      "  Client 4 training...\n",
      "  Client 5 training...\n",
      "  Aggregation complete.\n",
      "  Eval Round 10: T1 Acc=0.7185 | T2 F1=0.5296 | T3 MSE=0.00048\n",
      "\n",
      "STEP 8 (Federated Training) done.\n"
     ]
    }
   ],
   "source": [
    "# ------------------ STEP 8: Federated simulation (FedAvg) ------------------\n",
    "print(\"STEP 8: Federated simulation - FedAvg across clients...\")\n",
    "\n",
    "# --- Evaluation & Checkpoint helper ---\n",
    "WRITE_PATH = \"/kaggle/working/fl_model_output\"\n",
    "best_f1 = -1.0\n",
    "save_path = os.path.join(WRITE_PATH, \"best_global_model.keras\")\n",
    "ensure_dir(os.path.dirname(save_path))\n",
    "\n",
    "def evaluate_global_model(model, val_X, val_y1, val_y2, val_y3):\n",
    "    batch = 512\n",
    "    preds1 = []\n",
    "    preds2 = []\n",
    "    preds3 = [] # New for T3\n",
    "    \n",
    "    for i in range(0, val_X.shape[0], batch):\n",
    "        xb = val_X[i:i+batch]\n",
    "        # Model now returns 3 outputs\n",
    "        l1, l2, l3 = model(xb, training=False)\n",
    "        \n",
    "        p1 = np.argmax(l1.numpy(), axis=1)\n",
    "        p2 = np.argmax(l2.numpy(), axis=1)\n",
    "        p3 = l3.numpy() # Regression output (continuous)\n",
    "        \n",
    "        preds1.append(p1)\n",
    "        preds2.append(p2)\n",
    "        preds3.append(p3)\n",
    "        \n",
    "    preds1 = np.concatenate(preds1)\n",
    "    preds2 = np.concatenate(preds2)\n",
    "    preds3 = np.concatenate(preds3).flatten() # Flatten (N,1) -> (N,)\n",
    "    \n",
    "    # Task 1 Acc\n",
    "    fam_acc = accuracy_score(val_y1, preds1)\n",
    "    \n",
    "    # Task 2 Masked F1\n",
    "    mask = (val_y2 != -1)\n",
    "    if mask.sum() > 0:\n",
    "        sub_f1_macro = f1_score(val_y2[mask], preds2[mask], average='macro', zero_division=0)\n",
    "    else:\n",
    "        sub_f1_macro = 0.0\n",
    "\n",
    "    # Task 3 MSE (Mean Squared Error)\n",
    "    # We compare predicted magnitude vs true magnitude\n",
    "    mse_mag = np.mean((val_y3 - preds3) ** 2)\n",
    "        \n",
    "    return fam_acc, sub_f1_macro, mse_mag\n",
    "\n",
    "def checkpoint_model(model, current_f1, best_f1, save_path):\n",
    "    saved = False\n",
    "    # We still checkpoint based on T2 F1 as it's the hardest classification task\n",
    "    if current_f1 > best_f1:\n",
    "        print(f\"  Improved T2 macro-F1: {best_f1:.4f} -> {current_f1:.4f}. Saving model...\")\n",
    "        model.save(save_path, include_optimizer=False) \n",
    "        best_f1 = current_f1\n",
    "        saved = True\n",
    "    return best_f1, saved\n",
    "\n",
    "# --- Prepare TF datasets per client ---\n",
    "client_datasets = []\n",
    "client_sizes = []\n",
    "\n",
    "for i, c in enumerate(clients):\n",
    "    # UNPACK y3 HERE\n",
    "    Xc, y1c, y2c, y3c = c['X'], c['y1'], c['y2'], c['y3']\n",
    "    n = Xc.shape[0]\n",
    "    client_sizes.append(max(1, n))\n",
    "    \n",
    "    if n == 0:\n",
    "        client_datasets.append(None)\n",
    "        print(f\"Client {i}: empty dataset (will be skipped)\")\n",
    "        continue\n",
    "    \n",
    "    # ADD y3 TO DATASET\n",
    "    ds = tf.data.Dataset.from_tensor_slices((Xc, y1c, y2c, y3c))\n",
    "    ds = ds.shuffle(buffer_size=min(10000, n), seed=SEED)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    client_datasets.append(ds)\n",
    "    print(f\"Client {i}: dataset prepared, samples={n}\")\n",
    "\n",
    "# --- Federated training loop ---\n",
    "print(\"\\nStarting Federated Training (T1+T2+T3)...\")\n",
    "global_weights = get_model_weights(model)\n",
    "best_f1 = -1.0 \n",
    "\n",
    "for rnd in range(NUM_ROUNDS):\n",
    "    print(f\"\\n=== Fed Round {rnd+1}/{NUM_ROUNDS} ===\")\n",
    "    client_weights_list = []\n",
    "    client_effective_sizes = []\n",
    "\n",
    "    # --- Local Client Training ---\n",
    "    for i, ds_c in enumerate(client_datasets):\n",
    "        if ds_c is None or client_sizes[i] == 0:\n",
    "            continue\n",
    "            \n",
    "        # Set model to global state\n",
    "        set_model_weights(model, global_weights)\n",
    "        \n",
    "        # Client-side optimizer\n",
    "        client_opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "        \n",
    "        # Init optimizer vars\n",
    "        zero_grads = [tf.zeros_like(v) for v in model.trainable_variables]\n",
    "        client_opt.apply_gradients(zip(zero_grads, model.trainable_variables))\n",
    "\n",
    "        print(f\"  Client {i} training...\")\n",
    "        for epoch in range(EPOCHS_PER_CLIENT_ROUND):\n",
    "            # UNPACK y3 HERE\n",
    "            for xb, y1b, y2b, y3b in ds_c:\n",
    "                # PASS y3b TO TRAIN_STEP\n",
    "                l1_val, l2_val, l3_val, n_valid = train_step(xb, y1b, y2b, y3b, model, client_opt)\n",
    "            \n",
    "        # Collect updated weights\n",
    "        client_w = get_model_weights(model)\n",
    "        client_weights_list.append(client_w)\n",
    "        client_effective_sizes.append(float(client_sizes[i]))\n",
    "\n",
    "    # --- Federated Averaging (Aggregation) ---\n",
    "    if not client_weights_list:\n",
    "        print(\"No client updates collected this round.\")\n",
    "        continue\n",
    "\n",
    "    total_size = float(sum(client_effective_sizes))\n",
    "    new_global = []\n",
    "    \n",
    "    for weights_tuple in zip(*client_weights_list):\n",
    "        averaged = np.zeros_like(weights_tuple[0], dtype=np.float64)\n",
    "        for cw, size in zip(weights_tuple, client_effective_sizes):\n",
    "            averaged += (cw.astype(np.float64) * (size / total_size))\n",
    "        new_global.append(averaged.astype(np.float32))\n",
    "\n",
    "    global_weights = new_global\n",
    "    set_model_weights(model, global_weights)\n",
    "    print(\"  Aggregation complete.\")\n",
    "\n",
    "    # --- Evaluation & Checkpoint ---\n",
    "    # PASS test_y3 HERE\n",
    "    fam_acc, sub_f1_macro, mag_mse = evaluate_global_model(model, test_X, test_y1, test_y2, test_y3)\n",
    "    \n",
    "    print(f\"  Eval Round {rnd+1}: T1 Acc={fam_acc:.4f} | T2 F1={sub_f1_macro:.4f} | T3 MSE={mag_mse:.5f}\")\n",
    "    \n",
    "    best_f1, saved = checkpoint_model(model, sub_f1_macro, best_f1, save_path)\n",
    "\n",
    "print(\"\\nSTEP 8 (Federated Training) done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T17:46:25.367704Z",
     "iopub.status.busy": "2025-11-22T17:46:25.367412Z",
     "iopub.status.idle": "2025-11-22T17:46:47.655797Z",
     "shell.execute_reply": "2025-11-22T17:46:47.654952Z",
     "shell.execute_reply.started": "2025-11-22T17:46:25.367671Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 9: Evaluate final global model on held-out test set...\n",
      "Loading best saved model from checkpoint: /kaggle/working/fl_model_output/best_global_model.keras\n",
      "  Running predictions...\n",
      "\u001b[1m6306/6306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step\n",
      "\n",
      "==============================\n",
      "  Task 1 - Family Classification Report\n",
      "==============================\n",
      "  Task1 (family) accuracy on test: 0.7241\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Benign       0.97      0.79      0.87     37607\n",
      "      Spoofing       0.16      0.82      0.27      1744\n",
      "          DDoS       0.77      0.86      0.82   1117096\n",
      "           DoS       0.49      0.35      0.41    428312\n",
      "     Malformed       0.33      0.81      0.47      1747\n",
      "Reconnaissance       0.97      0.92      0.94     27676\n",
      "\n",
      "      accuracy                           0.72   1614182\n",
      "     macro avg       0.62      0.76      0.63   1614182\n",
      "  weighted avg       0.71      0.72      0.71   1614182\n",
      "\n",
      "\n",
      "==============================\n",
      "  Task 2 - Subtype Classification Report (Masked)\n",
      "==============================\n",
      "  Task2 (subtype) macro-F1 (masked, 1576575 rows): 0.5478\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "     ARP_Spoofing       0.78      0.45      0.57      1744\n",
      "MQTT-DDoS-Connect       0.99      1.00      0.99     41916\n",
      "MQTT-DDoS-Publish       0.97      0.17      0.29      8416\n",
      " MQTT-DoS-Connect       0.78      0.95      0.86      3131\n",
      " MQTT-DoS-Publish       0.54      1.00      0.70      8505\n",
      "   MQTT-Malformed       0.77      0.74      0.76      1747\n",
      "    Recon-OS_Scan       0.23      0.07      0.11      3834\n",
      " Recon-Ping_Sweep       0.10      0.99      0.18       186\n",
      "  Recon-Port_Scan       0.87      0.93      0.90     22622\n",
      "    Recon-VulScan       0.14      0.28      0.18      1034\n",
      " TCP_IP-DDoS-ICMP       0.78      1.00      0.88    349699\n",
      "  TCP_IP-DDoS-SYN       0.87      0.95      0.91    172397\n",
      "  TCP_IP-DDoS-TCP       0.69      1.00      0.82    182598\n",
      "  TCP_IP-DDoS-UDP       0.73      0.98      0.83    362070\n",
      "  TCP_IP-DoS-ICMP       0.53      0.00      0.01     98432\n",
      "   TCP_IP-DoS-SYN       0.91      0.75      0.82     98595\n",
      "   TCP_IP-DoS-TCP       0.72      0.00      0.01     82096\n",
      "   TCP_IP-DoS-UDP       0.32      0.02      0.04    137553\n",
      "\n",
      "         accuracy                           0.77   1576575\n",
      "        macro avg       0.65      0.63      0.55   1576575\n",
      "     weighted avg       0.72      0.77      0.68   1576575\n",
      "\n",
      "\n",
      "==============================\n",
      "  Task 3 - Magnitude Regression Report\n",
      "==============================\n",
      "  Scaled MSE: 0.00051 (Target range 0.0-1.0)\n",
      "  Mean Absolute Error (Original Scale): 0.2418\n",
      "\n",
      "  Sample Predictions (Real Scale):\n",
      "    True  |  Pred  |  Diff\n",
      "    ------------------------\n",
      "    10.00 | 10.04 |  0.04\n",
      "    10.39 | 10.35 |  0.04\n",
      "    10.00 | 10.07 |  0.07\n",
      "     9.17 |  9.35 |  0.19\n",
      "    10.48 | 10.75 |  0.26\n",
      "\n",
      "STEP 9 done.\n"
     ]
    }
   ],
   "source": [
    "# ------------------ STEP 9: Evaluate global model on test set ------------------\n",
    "print(\"STEP 9: Evaluate final global model on held-out test set...\")\n",
    "\n",
    "# <<< FIX: Load from the new .keras file path >>>\n",
    "WRITE_PATH = \"/kaggle/working/fl_model_output\" \n",
    "best_model_path = os.path.join(WRITE_PATH, \"best_global_model.keras\")\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"Loading best saved model from checkpoint: {best_model_path}\")\n",
    "    # We must provide custom_objects if we used custom loss, but here we used standard \n",
    "    # keras losses in the compile/train loop, so load_model usually works fine \n",
    "    # if the architecture is standard.\n",
    "    model = tf.keras.models.load_model(best_model_path)\n",
    "else:\n",
    "    print(\"No checkpointed model found, evaluating final round model.\")\n",
    "\n",
    "# Use model.predict for efficiency\n",
    "print(\"  Running predictions...\")\n",
    "pred_results = model.predict(test_X, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Unpack 3 outputs\n",
    "l1_all = np.asarray(pred_results[0])\n",
    "l2_all = np.asarray(pred_results[1])\n",
    "l3_all = np.asarray(pred_results[2]).flatten() # Task 3 (Regression)\n",
    "\n",
    "# Predicted labels (argmax for classification)\n",
    "y1_pred = np.argmax(l1_all, axis=-1).astype(np.int32)\n",
    "y2_pred = np.argmax(l2_all, axis=-1).astype(np.int32)\n",
    "\n",
    "# True labels\n",
    "y1_true = np.asarray(test_y1).astype(np.int32)\n",
    "y2_true = np.asarray(test_y2).astype(np.int32)\n",
    "y3_true = np.asarray(test_y3).astype(np.float32) # Scaled true values\n",
    "\n",
    "# --- Task1: family accuracy & report ---\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"  Task 1 - Family Classification Report\")\n",
    "print(\"=\"*30)\n",
    "fam_acc = accuracy_score(y1_true, y1_pred)\n",
    "print(f\"  Task1 (family) accuracy on test: {fam_acc:.4f}\\n\")\n",
    "task1_target_names = [class_map_task1.get(i, str(i)) for i in range(num_classes_task1)]\n",
    "print(classification_report(y1_true, y1_pred, target_names=task1_target_names, zero_division=0))\n",
    "\n",
    "# --- Task2: masked evaluation ---\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"  Task 2 - Subtype Classification Report (Masked)\")\n",
    "print(\"=\"*30)\n",
    "mask = (y2_true != -1)\n",
    "n_mask = int(np.sum(mask))\n",
    "if n_mask > 0:\n",
    "    y2_true_masked = y2_true[mask]\n",
    "    y2_pred_masked = y2_pred[mask]\n",
    "    \n",
    "    sub_f1_macro = f1_score(y2_true_masked, y2_pred_masked, average='macro', zero_division=0)\n",
    "    print(f\"  Task2 (subtype) macro-F1 (masked, {n_mask} rows): {sub_f1_macro:.4f}\\n\")\n",
    "    \n",
    "    present_labels = sorted(np.unique(y2_true_masked))\n",
    "    target_names = [class_map_task2.get(int(lbl), str(int(lbl))) for lbl in present_labels]\n",
    "    print(classification_report(y2_true_masked, y2_pred_masked, labels=present_labels, target_names=target_names, zero_division=0))\n",
    "else:\n",
    "    print(\"  No valid subtype rows in test (all benign or masked).\")\n",
    "\n",
    "# --- Task3: Regression evaluation ---\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"  Task 3 - Magnitude Regression Report\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# 1. Scaled metrics (MSE)\n",
    "mse_scaled = np.mean((y3_true - l3_all) ** 2)\n",
    "print(f\"  Scaled MSE: {mse_scaled:.5f} (Target range 0.0-1.0)\")\n",
    "\n",
    "# 2. Real-world metrics (MAE on original scale)\n",
    "# We use the 'scaler_y3' defined in STEP 4 to reverse the normalization\n",
    "if 'scaler_y3' in globals():\n",
    "    # Inverse transform\n",
    "    y3_true_real = scaler_y3.inverse_transform(y3_true.reshape(-1, 1)).flatten()\n",
    "    y3_pred_real = scaler_y3.inverse_transform(l3_all.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Calculate MAE\n",
    "    mae_real = np.mean(np.abs(y3_true_real - y3_pred_real))\n",
    "    print(f\"  Mean Absolute Error (Original Scale): {mae_real:.4f}\")\n",
    "    \n",
    "    print(\"\\n  Sample Predictions (Real Scale):\")\n",
    "    print(\"    True  |  Pred  |  Diff\")\n",
    "    print(\"    ------------------------\")\n",
    "    # Show 5 random samples\n",
    "    sample_idxs = np.random.choice(len(y3_true_real), 5, replace=False)\n",
    "    for idx in sample_idxs:\n",
    "        t_val = y3_true_real[idx]\n",
    "        p_val = y3_pred_real[idx]\n",
    "        print(f\"    {t_val:5.2f} | {p_val:5.2f} | {abs(t_val-p_val):5.2f}\")\n",
    "else:\n",
    "    print(\"  WARNING: 'scaler_y3' not found. Cannot calculate real-world MAE.\")\n",
    "\n",
    "print(\"\\nSTEP 9 done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T17:46:47.657008Z",
     "iopub.status.busy": "2025-11-22T17:46:47.656689Z",
     "iopub.status.idle": "2025-11-22T17:46:47.707973Z",
     "shell.execute_reply": "2025-11-22T17:46:47.707394Z",
     "shell.execute_reply.started": "2025-11-22T17:46:47.656990Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 10: Save final model & scalers...\n",
      "Saved model to /kaggle/working/fl_model_output/multitask_saved_final/joint_multitask_model.keras\n",
      "Saved feature scaler to /kaggle/working/fl_model_output/multitask_saved_final/scaler_minmax.joblib\n",
      "Saved T3 magnitude scaler to /kaggle/working/fl_model_output/multitask_saved_final/scaler_y3_magnitude.joblib\n",
      "STEP 10 done.\n",
      "\n",
      "ALL STEPS finished.\n"
     ]
    }
   ],
   "source": [
    "# ------------------ STEP 10: Save model & scaler ------------------\n",
    "print(\"STEP 10: Save final model & scalers...\")\n",
    "\n",
    "WRITE_PATH = \"/kaggle/working/fl_model_output\" \n",
    "save_dir = os.path.join(WRITE_PATH, \"multitask_saved_final\")\n",
    "ensure_dir(save_dir)\n",
    "\n",
    "# 1. Save Model\n",
    "model_save_path = os.path.join(save_dir, \"joint_multitask_model.keras\")\n",
    "model.save(model_save_path, include_optimizer=False)\n",
    "print(f\"Saved model to {model_save_path}\")\n",
    "\n",
    "# 2. Save Feature Scaler (X)\n",
    "scaler_save_path = os.path.join(save_dir, \"scaler_minmax.joblib\")\n",
    "joblib.dump(scaler, scaler_save_path)\n",
    "print(f\"Saved feature scaler to {scaler_save_path}\")\n",
    "\n",
    "# 3. Save Task 3 Target Scaler (y3)\n",
    "# CRITICAL: You need this to interpret the magnitude predictions later\n",
    "scaler_y3_save_path = os.path.join(save_dir, \"scaler_y3_magnitude.joblib\")\n",
    "if 'scaler_y3' in globals():\n",
    "    joblib.dump(scaler_y3, scaler_y3_save_path)\n",
    "    print(f\"Saved T3 magnitude scaler to {scaler_y3_save_path}\")\n",
    "else:\n",
    "    print(\"WARNING: scaler_y3 not found in globals(), could not save.\")\n",
    "\n",
    "print(\"STEP 10 done.\\n\")\n",
    "print(\"ALL STEPS finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8695623,
     "sourceId": 13675217,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
